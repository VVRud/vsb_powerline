{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\">Write own Attention Layer.</span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What I have</h1>\n",
    "\n",
    "**From signal**\n",
    "(everywhere can be used std_normalization):\n",
    "\n",
    "- Original signal\n",
    "- Denoised signal\n",
    "- Noise (difference between signals)\n",
    "- Mean of noiseless signal\n",
    "- Mean of original signal\n",
    "\n",
    "**From other data**:\n",
    "\n",
    "- Metrics of signal\n",
    "- Frequency domain of original signal\n",
    "- Spectrogram of signal, where channels are (denoised, original)\n",
    "\n",
    "<h1>What I should use</h1>\n",
    "\n",
    "- Denoised signal, using wavelet transform (800_000)\n",
    "    - **\\[Res(ODE)-Conv\\]+Pool x 2-4** -> **B-LSTM x 2-4** -> **Dense**<br>Put dropout and batch normalization in between.\n",
    "- Metrics of signal (36 * 128)\n",
    "    - **B-LSTM x 2**\n",
    "    - **Attention**\n",
    "    - **B-LSTM/Dense**\n",
    "- Spectrogram (129 x 27 x 128)\n",
    "    - **\\[Res(ODE)-Conv2D\\]+Pool x 4-8** -> **Dense**<br>Put dropout and batch normalization in between.\n",
    "- Frequency domain of a signal (1000 * 128)\n",
    "    - **\\[Res(ODE)-Conv\\]+Pool x 2-4** -> **B-LSTM x 2-4** -> **Dense**<br>Put dropout and batch normalization in between.\n",
    "    \n",
    "<h1>Final Model</h1>\n",
    "\n",
    "Input: stacked into vector (predicted probabilities of all models, \\[output of the last layer\\] x 5)\n",
    "- **Dense x 2-4**\n",
    "- **Gradient boosting**\n",
    "Output: final prediction\n",
    "\n",
    "Optimize threshhold for matthews correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:07.324929Z",
     "start_time": "2019-01-31T23:51:06.280430Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt as pw\n",
    "from scipy import fftpack, signal, stats\n",
    "import patsy\n",
    "from statsmodels.robust import mad\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from plotting import plot_phases, plot_single_func, plot_phases_func, plot_values, plot_values_loglog\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:07.380680Z",
     "start_time": "2019-01-31T23:51:07.373617Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "NUM_OF_MEASURES = 340\n",
    "COLS_STR = [str(i) for i in range(3 * NUM_OF_MEASURES)]\n",
    "COLS_INT = [i for i in range(3 * NUM_OF_MEASURES)]\n",
    "\n",
    "TR_PQ = '../data/parquet/train.parquet'\n",
    "TR_META = '../data/meta/metadata_train.csv'\n",
    "\n",
    "TS_PQ = '../data/parquet/test.parquet'\n",
    "TS_META = '../data/meta/metadata_test.csv'\n",
    "\n",
    "TR_H5 = '../data/hdf5/train.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:09:28.421379Z",
     "start_time": "2019-01-27T18:09:28.416177Z"
    }
   },
   "source": [
    "TR_PQ = '/home/vrudenko/Drive_data/PowerLine_Fault_Detection/data/parquet/train.parquet'\n",
    "TR_META = '/home/vrudenko/Drive_data/PowerLine_Fault_Detection/data/meta/metadata_train.csv'\n",
    "\n",
    "TS_PQ = '/home/vrudenko/Drive_data/PowerLine_Fault_Detection/data/parquet/test.parquet'\n",
    "TS_META = '/home/vrudenko/Drive_data/PowerLine_Fault_Detection/data/meta/metadata_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:24.130356Z",
     "start_time": "2019-01-31T23:51:10.163904Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pq.read_pandas(TR_PQ).to_pandas().values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:24.217032Z",
     "start_time": "2019-01-31T23:51:24.185861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(TR_META)\n",
    "test_meta = pd.read_csv(TS_META)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:24.377524Z",
     "start_time": "2019-01-31T23:51:24.373097Z"
    }
   },
   "outputs": [],
   "source": [
    "WAVELET_TYPE = 'db6'\n",
    "WAVELET_LEVEL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:24.543983Z",
     "start_time": "2019-01-31T23:51:24.532378Z"
    },
    "code_folding": [
     0,
     10,
     13,
     16,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def denoise_phase(phase):\n",
    "    wavelet = pw.Wavelet(WAVELET_TYPE)\n",
    "    wc = pw.wavedec(phase, wavelet, level=WAVELET_LEVEL)\n",
    "    sigma = mad(wc[-1])\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(phase)))\n",
    "\n",
    "    wc_r = wc[:]\n",
    "    wc_r[1:] = (pw.threshold(x, threshold) for x in wc[1:])\n",
    "    return pw.waverec(wc_r, wavelet)\n",
    "\n",
    "def std_normalize_phase(phase):\n",
    "    return (phase - np.mean(phase)) / np.std(phase)\n",
    "\n",
    "def denoise_normalize_phase(phase):\n",
    "    return std_normalize_phase(denoise_phase(phase))\n",
    "\n",
    "def minmax_normalize(phase):\n",
    "    return (phase - np.min(phase)) / (np.max(phase) - np.min(phase))\n",
    "\n",
    "def mean(phases):\n",
    "    return np.mean(phases, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:24.702392Z",
     "start_time": "2019-01-31T23:51:24.690310Z"
    },
    "code_folding": [
     0,
     7,
     11,
     14,
     17,
     23
    ]
   },
   "outputs": [],
   "source": [
    "def get_phases(df, measurement_id):\n",
    "    p1 = df[str(measurement_id * 3)].values\n",
    "    p2 = df[str(measurement_id * 3 + 1)].values\n",
    "    p3 = df[str(measurement_id * 3 + 2)].values\n",
    "\n",
    "    return p1, p2, p3\n",
    "\n",
    "def triple_denoise(df, measurement_id):\n",
    "    return np.asarray(\n",
    "        [denoise_phase(phase) for phase in get_phases(df, measurement_id)])\n",
    "\n",
    "def original_mean(df, measurement_id):\n",
    "    return mean(get_phases(df, measurement_id))\n",
    "\n",
    "def denoised_mean(df, measurement_id):\n",
    "    return mean(triple_denoise(df, measurement_id))\n",
    "\n",
    "def denoise_normalize(df, measurement_id):\n",
    "    return np.asarray([\n",
    "        std_normalize_phase(phase)\n",
    "        for phase in triple_denoise(df, measurement_id)\n",
    "    ])\n",
    "\n",
    "def original_normalize(df, measurement_id):\n",
    "    return np.asarray([\n",
    "        std_normalize_phase(phase) for phase in get_phases(df, measurement_id)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:24.852379Z",
     "start_time": "2019-01-31T23:51:24.845162Z"
    },
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def entropy(phase):\n",
    "    _, count = np.unique(phase, return_counts=True)\n",
    "    count = count / count.sum()\n",
    "    \n",
    "    return stats.entropy(count)\n",
    "\n",
    "def decomposition_energy(phase):\n",
    "    wavelet = pw.Wavelet(WAVELET_TYPE)\n",
    "    wc = pw.wavedec(phase, wavelet)\n",
    "    \n",
    "    return np.log10(np.sqrt(np.sum(np.array(wc[-WAVELET_LEVEL]) ** 2)) / len(wc[-WAVELET_LEVEL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:25.014175Z",
     "start_time": "2019-01-31T23:51:25.001454Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def metrics(phase, asdict=False):\n",
    "    f, Pxx = signal.welch(phase)\n",
    "    ix_mx = np.argmax(Pxx)\n",
    "    ix_mn = np.argmin(Pxx)\n",
    "\n",
    "    mean = np.mean(phase)\n",
    "    std = np.std(phase)\n",
    "    per_le = np.percentile(phase, [0, 1, 25, 50, 75, 99, 100])\n",
    "    \n",
    "    \n",
    "    d = {\n",
    "        'mean_signal': mean,\n",
    "        'std_signal': std,\n",
    "        'std_top_signal': mean + std,\n",
    "        'std_bot_signal': mean - std,\n",
    "        'kurtosis_signal': stats.kurtosis(phase),\n",
    "        'skewness_signal': stats.skew(phase),\n",
    "        'percentile_signal': per_le.tolist(),\n",
    "        'rel_percentile_signal': (per_le - mean).tolist(),\n",
    "        'range_signal': per_le[-1] - per_le[0],\n",
    "        \n",
    "        'entropy_signal': entropy(phase),\n",
    "        'dec_ener_signal': decomposition_energy(phase),\n",
    "        \n",
    "        'mean_amp': np.mean(Pxx),\n",
    "        'std_amp': np.std(Pxx),\n",
    "        'median_amp': np.median(Pxx),\n",
    "        'kurtosis_amp': stats.kurtosis(Pxx),\n",
    "        'skewness_amp': stats.skew(Pxx),\n",
    "        \n",
    "\n",
    "        'max_signal': np.max(phase),\n",
    "        'min_signal': np.min(phase),\n",
    "        \n",
    "        'max_amp': Pxx[ix_mx],\n",
    "        'min_amp': Pxx[ix_mn],\n",
    "        \n",
    "        'max_freq': f[ix_mx],\n",
    "        'min_freq': f[ix_mn],\n",
    "        \n",
    "        'strong_amp': np.sum(Pxx > 2.5),\n",
    "        'weak_amp': np.sum(Pxx < 0.4),\n",
    "    }\n",
    "\n",
    "    if asdict:\n",
    "        return d\n",
    "    else:\n",
    "        flat_list = []\n",
    "        for sublist in list(d.values()):\n",
    "            if isinstance(sublist, list):\n",
    "                for item in sublist:\n",
    "                    flat_list.append(item)\n",
    "            else:\n",
    "                flat_list.append(sublist)\n",
    "        return np.asarray(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:25.172091Z",
     "start_time": "2019-01-31T23:51:25.167781Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def onehot_phase(phase):\n",
    "    return [0 if phase == 0 else 1, 0 if phase == 1 else 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:25.335426Z",
     "start_time": "2019-01-31T23:51:25.328678Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def feature_matrix(phase, func, n_dims=128):\n",
    "    data = []\n",
    "    chunk = int(800000 / n_dims)\n",
    "    for part in range(n_dims):\n",
    "        data.append(func(phase[part * chunk:(part + 1) * chunk]))\n",
    "    return np.asarray(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:25.490854Z",
     "start_time": "2019-01-31T23:51:25.483112Z"
    },
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def get_freq(val, n=2002, d=(0.02 / 800000.)):\n",
    "    sig_fft = fftpack.fft(val, n=n)\n",
    "    sample_freq = fftpack.fftfreq(n=n, d=d)\n",
    "    pos_mask = np.where(sample_freq >= 0)\n",
    "\n",
    "    freqs = sample_freq[pos_mask][1:]\n",
    "    power = np.abs(sig_fft)[pos_mask][1:]\n",
    "\n",
    "    return freqs, power\n",
    "\n",
    "def get_freq_dom(values, denoised=None, n=1000, d=(0.02 / 800000.)):\n",
    "    size = n * 2 + 2\n",
    "    \n",
    "#     if denoised is None:\n",
    "#         denoised = denoise_phase(values)\n",
    "    \n",
    "    _, p1 = get_freq(values, size, d)\n",
    "#     _, p2 = get_freq(denoised, size, d)\n",
    "    \n",
    "#     return np.reshape(np.asarray([p1, p2]).T, (n, 2))\n",
    "    return p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:25.646194Z",
     "start_time": "2019-01-31T23:51:25.639468Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_spectrogram(values, denoised=None, fs=1 / (2e-2 / 800000), uselog=True):\n",
    "#     if denoised is None:\n",
    "#         denoised = denoise_phase(values)\n",
    "\n",
    "#     _, _, Sx1 = signal.spectrogram(denoised, fs)\n",
    "    _, _, ret = signal.spectrogram(values, fs)\n",
    "\n",
    "#     ret = np.concatenate((\n",
    "#         np.reshape(Sx1, (Sx1.shape[0], Sx1.shape[1], -1)),\n",
    "#         np.reshape(Sx2, (Sx2.shape[0], Sx2.shape[1], -1)),\n",
    "#     ),\n",
    "#                          axis=-1)\n",
    "\n",
    "    if uselog:\n",
    "        return np.log10(ret)\n",
    "    else:\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:51:25.808022Z",
     "start_time": "2019-01-31T23:51:25.799778Z"
    },
    "code_folding": [
     0,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "    cores.\n",
    "    \"\"\"        \n",
    "    # Effective axis where apply_along_axis() will be applied by each\n",
    "    # worker (any non-zero axis number would work, so as to allow the use\n",
    "    # of `np.array_split()`, which is only done on axis 0):\n",
    "    effective_axis = 1 if axis == 0 else axis\n",
    "    if effective_axis != axis:\n",
    "        arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "    # Chunks for the mapping (only a few chunks):\n",
    "    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "              for sub_arr in np.array_split(arr, mp.cpu_count())]\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.concatenate(individual_results)\n",
    "\n",
    "def unpacking_apply_along_axis(tp):\n",
    "    func1d, axis, arr, args, kwargs = tp\n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but and with arguments in a tuple\n",
    "    instead.\n",
    "\n",
    "    This function is useful with multiprocessing.Pool().map(): (1)\n",
    "    map() only handles functions that take a single argument, and (2)\n",
    "    this function can generally be imported from a module, as required\n",
    "    by map().\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T01:53:17.481606Z",
     "start_time": "2019-02-01T01:53:17.464131Z"
    },
    "code_folding": [
     0,
     22,
     25,
     31,
     37
    ]
   },
   "outputs": [],
   "source": [
    "def get_chunks(data, data_shape, file, dataset, chunk_size=256):\n",
    "    # computing number of chunks and extra chunks\n",
    "    chunks = (data_shape - data_shape % mp.cpu_count()) / chunk_size\n",
    "    extra = chunks % mp.cpu_count()\n",
    "    chunks -= extra\n",
    "\n",
    "    # computing ranges of data for each cpu\n",
    "    split = []\n",
    "    extend = 0\n",
    "    for i in range(mp.cpu_count()):\n",
    "        sta = chunk_size * chunks / mp.cpu_count() * i\n",
    "        end = chunk_size * chunks / mp.cpu_count() * (i + 1)\n",
    "        if i >= mp.cpu_count() - np.ceil(extra):\n",
    "            sta = sta + extend * chunk_size\n",
    "            end = data_shape if i == mp.cpu_count() - 1 else end + (extend + 1) * chunk_size\n",
    "            extend += 1\n",
    "        split.append((int(sta), int(end)))\n",
    "    \n",
    "    # splitting data\n",
    "    data = np.asarray(data)\n",
    "    masked = []\n",
    "    cpu_c = 0\n",
    "    for st, en in split:\n",
    "        mask = ((data >= st).astype(int) + (data < en).astype(int)) == 2\n",
    "        d = data[mask].tolist()\n",
    "        if d:\n",
    "            masked.append(tuple([d, file, dataset]))\n",
    "            cpu_c += 1\n",
    "    \n",
    "    return masked, cpu_c\n",
    "\n",
    "def unpacking_h5py_reading(tp):\n",
    "    ind, file, dataset = tp\n",
    "    with h5py.File(file, mode='r') as f:\n",
    "        ret = f[dataset][ind]\n",
    "    return ret\n",
    "\n",
    "def parallel_read_h5py_file(indicies, shape, path, dataset, chunk_size=256):\n",
    "    chunks, cpu_c = get_chunks(indicies, shape, path, dataset, chunk_size)\n",
    "    pool = mp.Pool(cpu_c)\n",
    "    individual_results = pool.map(unpacking_h5py_reading, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.concatenate(individual_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T23:52:24.698555Z",
     "start_time": "2019-01-31T23:52:24.673217Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_process_to_h5(name, data, dims=128, batch=512):\n",
    "    steps = int(np.ceil(data.shape[0] / batch))\n",
    "\n",
    "    dir_file = '../data/hdf5/' + name + '.hdf5'\n",
    "    print('Creating hdf5 file in directory: {}'.format(dir_file))\n",
    "    hd_file = h5py.File(dir_file, mode='a')\n",
    "    \n",
    "    print('Creating datasets: ', end='')\n",
    "    if 'denoised' not in hd_file.keys():\n",
    "        denoised_ds = hd_file.create_dataset(\n",
    "            'denoised', shape=data.shape, dtype=np.float32, chunks=(256, 800000))\n",
    "        print('denoised, ', end='')\n",
    "    else:\n",
    "        denoised_ds = hd_file['denoised']\n",
    "    \n",
    "    if 'stats' not in hd_file.keys():\n",
    "        metrics_ds = hd_file.create_dataset(\n",
    "            'stats',\n",
    "            shape=(data.shape[0], dims, 36),\n",
    "            dtype=np.float32,\n",
    "            chunks=(256, dims, 36))\n",
    "        print('stats, ', end='')\n",
    "    else:\n",
    "        metrics_ds = hd_file['stats']\n",
    "\n",
    "    if 'spectr' not in hd_file.keys():\n",
    "        spectrogram_ds = hd_file.create_dataset(\n",
    "            'spectr',\n",
    "            shape=(data.shape[0], dims, 129, 27),\n",
    "            dtype=np.float32,\n",
    "            chunks=(256, dims, 129, 27))\n",
    "        print('spectr, ', end='')\n",
    "    else:\n",
    "        spectrogram_ds = hd_file['spectr']\n",
    "\n",
    "    if 'freq' not in hd_file.keys():\n",
    "        freq_dom_ds = hd_file.create_dataset(\n",
    "            'freq',\n",
    "            shape=(data.shape[0], dims, 1000),\n",
    "            dtype=np.float32,\n",
    "            chunks=(256, dims, 1000))\n",
    "        print('freq.')\n",
    "    else:\n",
    "        freq_dom_ds = hd_file['freq']\n",
    "\n",
    "    t = tnrange(steps)\n",
    "    for i in t:\n",
    "        start = i * batch\n",
    "        finish = (i + 1) * batch if i + 1 != steps else data.shape[0]\n",
    "\n",
    "        t.set_description('Denoising')\n",
    "        den_data = parallel_apply_along_axis(denoise_phase, 1,\n",
    "                                             data[start:finish])\n",
    "        denoised_ds[start:finish] = parallel_apply_along_axis(\n",
    "            std_normalize_phase, 1, den_data)\n",
    "\n",
    "        t.set_description('Metrics')\n",
    "        metrics_ds[start:finish] = parallel_apply_along_axis(\n",
    "            feature_matrix, 1, den_data, func=metrics, n_dims=dims)\n",
    "\n",
    "        t.set_description('Spectrogram')\n",
    "        spectrogram_ds[start:finish] = parallel_apply_along_axis(\n",
    "            feature_matrix, 1, den_data, func=get_spectrogram, n_dims=dims)\n",
    "\n",
    "        t.set_description('Frequency')\n",
    "        freq_dom_ds[start:finish] = parallel_apply_along_axis(\n",
    "            feature_matrix, 1, den_data, func=get_freq_dom, n_dims=dims)\n",
    "\n",
    "        del den_data\n",
    "        \n",
    "    hd_file.flush()\n",
    "    hd_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T00:34:36.982346Z",
     "start_time": "2019-01-31T23:52:25.454472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating hdf5 file in directory: ../data/hdf5/train.hdf5\n",
      "Creating datasets: denoised, stats, spectr, freq.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e64aef7119e4d30aff64c00950b34a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parallel_process_to_h5('train', train, batch=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:28:15.929356Z",
     "start_time": "2019-01-27T01:28:15.925759Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def diff(df, measurement_id):\n",
    "    phase = df[measurement_id]\n",
    "    \n",
    "    return phase[1:] - phase[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T11:19:34.843417Z",
     "start_time": "2019-01-27T11:19:34.525512Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_phases(train, train_meta, 0, plot_range=[500000, 510000], url=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:36:47.570718Z",
     "start_time": "2019-01-27T01:36:30.102357Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_phases_func(\n",
    "    train, train_meta, 0, func=triple_denoise, name='denoise', url=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T19:44:19.981141Z",
     "start_time": "2019-01-15T19:44:19.916610Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "denoise_phase(train['0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:29:51.287398Z",
     "start_time": "2019-01-27T01:29:46.834719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_single_func(\n",
    "    train, train_meta, 0, func=diff, name='diff', url=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T19:42:09.554583Z",
     "start_time": "2019-01-15T19:42:01.753814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_single_func(\n",
    "    train, train_meta, 0, func=denoised_mean, name='wavelet_mean', url=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T00:55:13.415943Z",
     "start_time": "2019-02-01T00:55:09.701101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import Sequence\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T01:53:19.993989Z",
     "start_time": "2019-02-01T01:53:19.964313Z"
    },
    "code_folding": [
     1,
     36,
     40,
     50,
     70,
     92,
     114
    ]
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 meta,\n",
    "                 to_gen,\n",
    "                 data_shape,\n",
    "                 do_und_sam,\n",
    "                 data=None,\n",
    "                 file_path=None,\n",
    "                 first=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.meta = meta\n",
    "        self.to_gen = [element.lower() for element in to_gen]\n",
    "        self.data_shape = data_shape\n",
    "        self.do_und_sam = do_und_sam\n",
    "\n",
    "        self.first = first\n",
    "\n",
    "        print('{} will be returned.'.format(to_gen))\n",
    "        if file_path is not None and os.path.exists(file_path):\n",
    "            self.file_path = file_path\n",
    "            self.cache = True\n",
    "            print('Using cached data.')\n",
    "        elif data is not None:\n",
    "            self.data = data\n",
    "            self.cache = False\n",
    "            print(\n",
    "                'Generating data on fly.\\n*IT CAN BE EXTREMELY SLOW AND WILL SLOWDOWN TRAINING*'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'data and file_path can not be None simultaneously. Set valid file_path or feed in valid data.'\n",
    "            )\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.dat_ind) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Get indexes of the batch\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size if index + 1 < self.__len__() else len(self.dat_ind)\n",
    "        ind = self.dat_ind[start:end]\n",
    "\n",
    "        # Generate data\n",
    "        return self.__data_generation(ind)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        # Undersampling\n",
    "        if self.do_und_sam:\n",
    "            under_sampler = RandomUnderSampler(\n",
    "                sampling_strategy='majority',\n",
    "                random_state=np.random.randint(0, high=1024, size=1)[0])\n",
    "            resampled = under_sampler.fit_resample(\n",
    "                self.meta['signal_id'].values.reshape((-1, 1)),\n",
    "                self.meta['target'].values.reshape((-1, 1)),\n",
    "            )\n",
    "            ind, _ = shuffle(resampled[0], resampled[1])\n",
    "        else:\n",
    "            ind, _ = shuffle(self.meta['signal_id'].values.reshape((-1, 1)),\n",
    "                             self.meta['signal_id'].values.reshape((-1, 1)))\n",
    "        # Resampling data indicies\n",
    "        # In test data indicies starts from 8712, so we should fit them from 0 to *some value*\n",
    "        # so that we could correctly get data from hdf5 file or pure dataset\n",
    "        self.dat_ind = (ind - self.first)\n",
    "\n",
    "    def __data_generation(self, ind):\n",
    "        'Generates data containing batch_size samples'\n",
    "        if self.cache:\n",
    "            # Get data from file\n",
    "            ind = np.asarray(ind).ravel()\n",
    "            ind.sort()\n",
    "            ind = ind.tolist()\n",
    "\n",
    "            lists = self.__get_cached(ind)\n",
    "            targets = self.meta['target'].values[ind]\n",
    "\n",
    "            X = []\n",
    "            state = np.random.randint(0, high=1024, size=1)[0]\n",
    "            for l in lists:\n",
    "                X_, y = shuffle(l, targets, random_state=state)\n",
    "                X.append(X_)\n",
    "        else:\n",
    "            X = self.__generate_on_fly(ind)\n",
    "            y = self.meta['target'].values[ind]\n",
    "\n",
    "        return [X, y]\n",
    "\n",
    "    def __get_cached(self, ind):\n",
    "        'Get cached data from hdf5 file'\n",
    "        X = []\n",
    "        if 'denoised' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_read_h5py_file(ind, self.data_shape, self.file_path,\n",
    "                                   'denoised'))\n",
    "        if 'stats' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_read_h5py_file(ind, self.data_shape, self.file_path,\n",
    "                                   'stats'))\n",
    "        if 'spectr' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_read_h5py_file(ind, self.data_shape, self.file_path,\n",
    "                                   'spectr'))\n",
    "        if 'freq' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_read_h5py_file(ind, self.data_shape, self.file_path,\n",
    "                                   'freq'))\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __generate_on_fly(self, ind):\n",
    "        'Generate data on fly'\n",
    "        prep = self.data[ind]\n",
    "        X = []\n",
    "        if 'denoised' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_apply_along_axis(denoise_normalize_phase, 1, prep))\n",
    "        if 'stats' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_apply_along_axis(\n",
    "                    feature_matrix, 1, X[0], func=metrics).transpose((0, 2,\n",
    "                                                                      1)))\n",
    "        if 'spectr' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_apply_along_axis(\n",
    "                    feature_matrix, 1, X[0], func=get_spectrogram).transpose(\n",
    "                        (0, 2, 3, 1)))\n",
    "        if 'freq' in self.to_gen:\n",
    "            X.append(\n",
    "                parallel_apply_along_axis(\n",
    "                    feature_matrix, 1, X[0], func=get_freq_dom).transpose(\n",
    "                        (0, 2, 1)))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:54:07.683575Z",
     "start_time": "2019-01-31T22:54:07.667523Z"
    },
    "code_folding": [
     1,
     19,
     40,
     43,
     66
    ]
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:42:36.997431Z",
     "start_time": "2019-01-31T22:42:36.986368Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import (Bidirectional, LSTM, Dense, TimeDistributed, Conv1D,\n",
    "                          Input, Add, BatchNormalization, ReLU, Dropout, Flatten, Activation, Layer)\n",
    "from keras.models import Model\n",
    "from keras.callbacks import (TensorBoard, ModelCheckpoint)\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:05:49.009946Z",
     "start_time": "2019-01-31T22:05:49.001722Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T20:48:29.324570Z",
     "start_time": "2019-01-27T20:48:29.316659Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def res_layer(input_data, filters, kernel_size, block_num=None):\n",
    "    if block_num is None:\n",
    "        raise ValueError('Block number is not defined')\n",
    "\n",
    "    block = 'res_block' + str(block_num) + '_'\n",
    "    inp_ = input_data\n",
    "\n",
    "    out = Conv1D(\n",
    "        filters, kernel_size, padding='same', name=block + 'conv1')(input_data)\n",
    "    out = BatchNormalization(name=block + 'bn1')(out)\n",
    "    out = ReLU(name=block + 'relu1')(out)\n",
    "\n",
    "    #     out = Dropout(drop, name=block + 'drop')(out)\n",
    "\n",
    "    out = Conv1D(\n",
    "        filters, kernel_size, padding='same', name=block + 'conv2')(out)\n",
    "    out = BatchNormalization(name=block + 'bn2')(out)\n",
    "\n",
    "    out = Add(name=block + 'add')([out, inp_])\n",
    "    out = ReLU(name=block + 'relu2')(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def den_branch(inp):\n",
    "    # Conv1D -> B-LSTM -> Attention -> Dense\n",
    "    pass\n",
    "\n",
    "def stats_branch(inp):\n",
    "    # B-LSTM -> Attention -> Dense\n",
    "    pass\n",
    "\n",
    "def spectr_branch(inp):\n",
    "    # Conv2D > Dense\n",
    "    pass\n",
    "\n",
    "def freq_branch(inp):\n",
    "    # Conv1D -> B-LSTM -> Attention -> Dense\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T21:54:30.551755Z",
     "start_time": "2019-01-30T21:54:30.352970Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "denoised = Input(shape=(800000, 1), name='Denoised')\n",
    "stats = Input(shape=(36, 128), name='Statistics')\n",
    "spectr = Input(shape=(129, 27, 128, 2), name='Spectrogram')\n",
    "freq = Input(shape=(1000, 128, 2), name='Frequencies')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[denoised, stats, spectr, freq], outputs=[output_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:35:41.816651Z",
     "start_time": "2019-01-27T18:35:41.786344Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model(filters, kernel, True)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', matthews_correlation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:36:36.863505Z",
     "start_time": "2019-01-27T18:36:36.858354Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def callbacks(model_name, batch_size):\n",
    "    m_tb = TensorBoard(\n",
    "        log_dir='../logs/{}'.format(model_name),\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_grads=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    m_ch = ModelCheckpoint(\n",
    "        filepath='../weights/{}/{epoch:02d}-{val_loss:.2f}.hdf5'.format(model_name),\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        period=5\n",
    "    )\n",
    "    \n",
    "    return [m_tb, m_ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T20:34:58.446366Z",
     "start_time": "2019-01-24T20:23:04.825214Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks('model_1', 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "982px",
    "right": "20px",
    "top": "111px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
